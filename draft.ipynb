{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from occupancy.datasets.nuscenes import NuScenesDataset, NuScenesOccupancyDataset\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "nusc = NuScenesOccupancyDataset(\n",
    "    data_dir='/mnt/f/datasets/nuscenes/nuScenes-Occupancy-v0.1/', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occ_shuffle(occ: Tensor, cube_size: int = 32, shuffle_rato: float = 0.2):\n",
    "    total_cubes = (occ.shape[-3] // cube_size) * (occ.shape[-2] // cube_size) * (occ.shape[-1] // cube_size)\n",
    "    total_shuffle = int(total_cubes * shuffle_rato)\n",
    "    cubes = torch.zeros(occ.shape[0], occ.shape[1], total_cubes, cube_size, cube_size, cube_size)\n",
    "    for i in range(occ.shape[-3] // cube_size):\n",
    "        for j in range(occ.shape[-2] // cube_size):\n",
    "            for k in range(occ.shape[-1] // cube_size):\n",
    "                ind = i * (occ.shape[-2] // cube_size) * (occ.shape[-1] // cube_size) + j * (occ.shape[-1] // cube_size) + k\n",
    "                cubes[:, :, ind] = occ[:, :, i * cube_size:(i + 1) * cube_size, j * cube_size:(j + 1) * cube_size, k * cube_size:(k + 1) * cube_size] \n",
    "    ind_to_shuffle = torch.randperm(total_cubes)[:total_shuffle]\n",
    "    shuffle_ind = torch.randperm(total_shuffle)\n",
    "    cubes[:, :, ind_to_shuffle] = cubes[:, :, ind_to_shuffle][:, :, shuffle_ind]\n",
    "    shuffled = torch.zeros_like(occ)\n",
    "    for i in range(occ.shape[-3] // cube_size):\n",
    "        for j in range(occ.shape[-2] // cube_size):\n",
    "            for k in range(occ.shape[-1] // cube_size):\n",
    "                ind = i * (occ.shape[-2] // cube_size) * (occ.shape[-1] // cube_size) + j * (occ.shape[-1] // cube_size) + k\n",
    "                shuffled[:, :, i*cube_size:(i+1)*cube_size, j*cube_size:(j+1)*cube_size, k*cube_size:(k+1)*cube_size] = cubes[:, :, ind]\n",
    "    return shuffled\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = nusc[12]\n",
    "occ = F.interpolate(occ.unsqueeze(0).float(), scale_factor=2, mode='trilinear', align_corners=True).squeeze(0).argmax(0)\n",
    "occ = F.one_hot(occ, num_classes=18).permute(3, 0, 1, 2).argmax(0)\n",
    "occ = occ_shuffle(occ[None, None, ...], 64,0.2)[0, 0]\n",
    "i,j,k = torch.where(occ)\n",
    "c = occ[i,j,k]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(i,j,k, c=c, marker='s', s=1)\n",
    "ax.set_xlim(0, occ.shape[-3])\n",
    "ax.set_ylim(0, occ.shape[-2])\n",
    "ax.set_zlim(0, occ.shape[-1])\n",
    "ax.set_box_aspect((1,1,occ.shape[-1]/occ.shape[-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained('stabilityai/sdxl-vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from occupancy.datasets.nuscenes import NuScenesDataset, NuScenesOccupancyDataset\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "nusc = NuScenesDataset(\n",
    "    data_dir='/mnt/f/datasets/nuscenes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "import torch\n",
    "\n",
    "data = nusc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ0 = data.lidar_top.occupancy\n",
    "occ0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_occ(occ):\n",
    "    with plt.ioff():\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        i,j,k = torch.where(occ[0, 0])\n",
    "        ax.scatter(i,j,k, marker='s', s=1, c=k)\n",
    "        ax.set_xlim(0, occ.shape[-3])\n",
    "        ax.set_ylim(0, occ.shape[-2])\n",
    "        ax.set_zlim(0, occ.shape[-1])\n",
    "        ax.set_box_aspect((1,1,occ.shape[-1]/occ.shape[-2]))\n",
    "        return fig\n",
    "\n",
    "occ0_fig = show_occ(occ0)\n",
    "occ0_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_kernel(size: int, sigma: float) -> torch.Tensor:\n",
    "    grid = torch.arange(size, dtype=torch.float32)\n",
    "    grid -= (size - 1) / 2\n",
    "    grid = grid / sigma\n",
    "    grid = grid ** 2\n",
    "    grid = -0.5 * grid\n",
    "    grid = torch.exp(grid)\n",
    "    grid = grid / grid.sum()\n",
    "    return grid\n",
    "\n",
    "def occ_approx_roi(occ):\n",
    "    #occ = occ.argmax(dim=1, keepdim=True)\n",
    "    occ = occ > 0\n",
    "    gaussian_blur_kernel3d = build_kernel(7, 1)\n",
    "    gaussian_blur_kernel3d = gaussian_blur_kernel3d[None, :] * gaussian_blur_kernel3d[:, None] * gaussian_blur_kernel3d[:, None, None]\n",
    "    gaussian_blur_kernel3d = gaussian_blur_kernel3d / gaussian_blur_kernel3d.sum()\n",
    "    gaussian_blur_kernel3d = gaussian_blur_kernel3d[None, None, ...]\n",
    "    mask = F.conv3d(occ.float(), gaussian_blur_kernel3d.to(device=occ.device), padding=3)\n",
    "    mask = mask > 0\n",
    "    mask = mask.any(dim=-1, keepdim=True).expand_as(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dinov2 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\", trust_repo=True, skip_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportable = torch.export.export(dinov2, (torch.rand(1, 3, 224, 224),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ1 = occ_approx_roi(occ0.cuda()).cpu()\n",
    "#occ1 = occ1.any(dim=-1, keepdim=True).expand_as(occ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(occ1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./.torch/facebookresearch_dinov2_main\n",
      "/home/zc2309/mambaforge/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/zc2309/workspace/occupancy/./.torch/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/zc2309/workspace/occupancy/./.torch/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/zc2309/workspace/occupancy/./.torch/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import nn, Tensor\n",
    "torch.hub.set_dir(os.path.join(os.curdir, \".torch\"))\n",
    "image_feature = torch.hub.load(\n",
    "    \"facebookresearch/dinov2\", \"dinov2_vitb14\", trust_repo=True, skip_validation=True\n",
    ")\n",
    "image_feature.eval()\n",
    "image_feature = image_feature.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from occupancy.pipelines.panoramic2voxel import LinearCategoricalDeformation\n",
    "from occupancy.models.transformer import ConditionalDecoderLayer, DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 22.7500, -29.7500,  20.0000,  ..., -24.7500,   3.9844,  28.6250],\n",
       "         [-51.2500, -28.8750,  24.0000,  ...,  28.3750,  -4.9062,   3.3281],\n",
       "         [-56.5000,  -4.9688,   7.5625,  ...,  17.0000, -15.8750,  -4.9375],\n",
       "         ...,\n",
       "         [ 33.2500, -30.3750, -19.5000,  ..., -30.5000,  16.8750,  -7.7500],\n",
       "         [ 38.0000, -39.5000,   5.4688,  ..., -22.6250,   2.4219,   3.7969],\n",
       "         [ 36.2500, -35.0000,  -3.8906,  ..., -11.3750, -36.2500,  51.7500]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "class BEVLinearCategoricalDeformation(nn.Module):\n",
    "    def __init__(self, feature_extrator):\n",
    "        super().__init__()\n",
    "        self.feature_extrator = feature_extrator\n",
    "        for i, block in enumerate(self.feature_extrator.blocks):\n",
    "            setattr(block, \"_block_index\", i)\n",
    "            block.register_forward_hook(self._bev_linear_categorical_deformation_hook)\n",
    "        hidden_size = 768\n",
    "        df_size = 32 * 32 * 4\n",
    "        self.num_layers = len(self.feature_extrator.blocks)\n",
    "        self._bev_features = defaultdict(list)\n",
    "        self._last_feats = [None for _ in range(self.num_layers)]\n",
    "        self.deformations = nn.ModuleList([LinearCategoricalDeformation(hidden_size, hidden_size, deformative_size=df_size) for _ in range(self.num_layers)])\n",
    "        self.attentions = nn.ModuleList([ConditionalDecoderLayer(hidden_size, hidden_size // 64, 64) for _ in range(self.num_layers - 1)])\n",
    "        \n",
    "    \n",
    "    def _bev_linear_categorical_deformation_hook(self, module, input, output):\n",
    "        block_index = getattr(module, \"_block_index\")\n",
    "        df_feat = self.deformations[block_index](output)\n",
    "        self._bev_features[block_index].append(df_feat)\n",
    "        if len(self._bev_features[block_index]) == 6:\n",
    "            bev_feat = sum(self._bev_features[block_index])\n",
    "            self._last_feats[block_index] = bev_feat\n",
    "            self._bev_features[block_index] = []\n",
    "            if block_index > 0:\n",
    "                self._last_feats[block_index] = self.attentions[block_index - 1](self._last_feats[block_index - 1], self._last_feats[block_index])\n",
    "                self._last_feats[block_index - 1] = None\n",
    "                \n",
    "                \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for i in x.unbind(1):\n",
    "            self.feature_extrator(i)\n",
    "        return self._last_feats.pop()\n",
    "    \n",
    "image_feature.to('cuda')\n",
    "image_feature.requires_grad_(False)\n",
    "model = BEVLinearCategoricalDeformation(image_feature)\n",
    "model.to('cuda').bfloat16()\n",
    "x = torch.rand(1, 6, 3, 518, 518).cuda().bfloat16()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._last_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from occupancy import ops\n",
    "def occ_approx_roi(occ: Tensor):\n",
    "    \n",
    "    occ = occ.argmax(0)\n",
    "    occ = occ != 0\n",
    "    F.gaussian_blur(occ.float(), kernel_size=3, sigma=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vitl14.forward_features(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from occupancy.pipelines.autoencoderkl_3d import AutoEncoderKL3d\n",
    "import torch_tensorrt\n",
    "model = AutoEncoderKL3d(18, 18, 64, 64, 2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vitl14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "os.chdir('/home/zc2309/workspace/occupancy/Depth_Anything')\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "from torchvision.transforms import Compose\n",
    "model = DepthAnything.from_pretrained(\"LiheYoung/depth_anything_vitl14\")\n",
    "\n",
    "transform = Compose([\n",
    "        Resize(\n",
    "            width=518,\n",
    "            height=518,\n",
    "            resize_target=False,\n",
    "            keep_aspect_ratio=True,\n",
    "            ensure_multiple_of=14,\n",
    "            resize_method='lower_bound',\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        PrepareForNet(),\n",
    "    ])\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = np.array(image) / 255.0\n",
    "image = transform({'image': image})['image']\n",
    "image = torch.from_numpy(image).unsqueeze(0).cuda()\n",
    "model.cuda()\n",
    "\n",
    "depth = model(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(TF.to_pil_image(depth[0].cpu(), mode='F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.export.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from occupancy.ops import view_as_cartesian, view_as_polar\n",
    "\n",
    "\n",
    "x = torch.rand(1, 1, 64, 64, 64)\n",
    "z = view_as_polar(x, (512, 512, 512), mode='bilinear')\n",
    "x_ = view_as_cartesian(z, (64, 64, 64), mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x - x_).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
